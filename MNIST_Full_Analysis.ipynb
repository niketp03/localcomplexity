{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as ch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler, AdamW\n",
    "from torch.func import jacfwd, jacrev\n",
    "from torch import vmap\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch as ch\n",
    "import einops\n",
    "\n",
    "from attacks import PGD\n",
    "\n",
    "import ml_collections\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "  \"\"\"hyperparameter configuration.\"\"\"\n",
    "  config = ml_collections.ConfigDict()\n",
    "\n",
    "  config.optimizer = 'adam'\n",
    "  config.lr = 1e-4\n",
    "  config.momentum = 0.01\n",
    "  config.bias_decay = False\n",
    "\n",
    "  config.train_batch_size = 64 #256\n",
    "  config.test_batch_size = 64 #256\n",
    "\n",
    "  config.num_steps = 1000000                       # number of training steps\n",
    "  config.weight_decay = 0.01\n",
    "\n",
    "  config.label_smoothing = 0.0\n",
    "\n",
    "  config.log_steps = np.unique(\n",
    "      np.logspace(0,np.log10(config.num_steps),50).astype(int).clip(\n",
    "          0,config.num_steps\n",
    "          )\n",
    "      )\n",
    "  \n",
    "  config.seed = 42\n",
    "\n",
    "  #config.dmax = 1\n",
    "  #config.dmin = 0\n",
    "\n",
    "  config.wandb_proj = 'LRLC_study_MNIST_MLP'\n",
    "  config.wandb_pref = 'MNIST-MLP'\n",
    "\n",
    "  config.resume_step = 0\n",
    "\n",
    "  ## mlp params\n",
    "  config.input_dim = 784\n",
    "  config.output_dim = 10\n",
    "  config.hidden_dim = 200\n",
    "  config.n_layers = 4\n",
    "  config.input_weights_init_scale = np.sqrt(2) #sets initialization standard deviation to be = (input_weights_init_scale)/sqrt(input_dim) \n",
    "  config.output_weights_init_scale = np.sqrt(2)\n",
    "\n",
    "\n",
    "  ## local complexity approx. parameters\n",
    "  config.compute_LC = True\n",
    "  config.n_batches = 2\n",
    "  config.sigma = 0.01\n",
    "  config.n_iters_LC = 1\n",
    "\n",
    "  ## adv robustness parameters\n",
    "  config.compute_robust = True                   # note that if normalize==True, data is not bounded between [0,1]\n",
    "  config.atk_eps = 80/255   ## 8/255\n",
    "  config.atk_alpha = 10/255  ## 2/255\n",
    "  config.atk_itrs = 10\n",
    "  config.dmin = -2.0\n",
    "  config.dmax = 2.0\n",
    "\n",
    "  ## data rank evaluation\n",
    "  config.eval_data_rank = True\n",
    "  config.n_batchs_for_rank = 10\n",
    "\n",
    "  ## data local rank evaluation\n",
    "  config.local_rank_eval = True\n",
    "  config.n_batchs_for_local_rank = 1\n",
    "\n",
    "  ##\n",
    "  config.save_model = True\n",
    "  config.model_save_dir = 'models'\n",
    "\n",
    "  return config\n",
    "\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 35.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 840kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 7.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 17.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../mnist_data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('../mnist_data',\n",
    "                                           download=True,\n",
    "                                           train=True,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                               transforms.Normalize((0.1307,), (0.3081,)), # normalize inputs\n",
    "                                               lambda x: einops.rearrange(x, 'c h w -> (c h w)') # flatten the input images\n",
    "                                           ]))\n",
    "idx = list(range(len(train_dataset)))\n",
    "random.shuffle(idx)\n",
    "train_dataset = Subset(train_dataset, idx[:1000])\n",
    "\n",
    "train_loader = ch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=config.train_batch_size,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST('../mnist_data',\n",
    "                                           download=True,\n",
    "                                           train=False,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                               transforms.Normalize((0.1307,), (0.3081,)), # normalize inputs\n",
    "                                               lambda x: einops.rearrange(x, 'c h w -> (c h w)') # flatten the input images\n",
    "                                           ]))\n",
    "idx = list(range(len(test_dataset)))\n",
    "random.shuffle(idx)\n",
    "train_dataset = Subset(test_dataset, idx[:1000])\n",
    "\n",
    "test_loader = ch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=config.train_batch_size,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, input_std_init=1, output_std_init=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Define input layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.initial_weights = []\n",
    "        \n",
    "        # Input layer\n",
    "        input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        initial_init = ch.normal(mean=ch.zeros(hidden_dim, input_dim), std=input_std_init / ch.sqrt(ch.tensor(input_dim)))\n",
    "        input_layer.weight = nn.parameter.Parameter(initial_init.clone())\n",
    "        input_layer.bias = nn.parameter.Parameter(ch.zeros(hidden_dim))\n",
    "        self.layers.append(input_layer)\n",
    "        self.input_layer = input_layer\n",
    "        self.initial_weights.append(initial_init)\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "            fan_in = self.layers[-1].weight.shape[1]\n",
    "            hidden_init = ch.normal(mean=ch.zeros(hidden_dim, hidden_dim), std=output_std_init / ch.sqrt(ch.tensor(fan_in)))\n",
    "            hidden_layer.weight = nn.parameter.Parameter(hidden_init.clone())\n",
    "            hidden_layer.bias = nn.parameter.Parameter(ch.zeros(hidden_dim))\n",
    "            self.layers.append(hidden_layer)\n",
    "            self.initial_weights.append(hidden_init)\n",
    "\n",
    "        # Define output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.output_init = ch.normal(mean=ch.zeros(output_dim, hidden_dim), std=output_std_init / ch.sqrt(ch.tensor(hidden_dim)))\n",
    "        self.output_layer.weight = nn.parameter.Parameter(self.output_init.clone())\n",
    "        self.output_layer.bias = nn.parameter.Parameter(ch.zeros(output_dim))\n",
    "        \n",
    "        # Activation functions\n",
    "        self.input_activation = nn.ReLU()\n",
    "        self.output_activation = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = einops.rearrange(x, \"b c u i -> b c (u i)\")\n",
    "        for layer in self.layers:\n",
    "            x = self.input_activation(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = self.output_activation(x)\n",
    "        #x = einops.rearrange(x, \"x 1 n -> x n\")\n",
    "        return x.float()\n",
    "    \n",
    "    def forward_to(self, x, n_layer):\n",
    "        #x = einops.rearrange(x, \"b c u i -> b c (u i)\")\n",
    "        for layer in self.layers[:n_layer]:\n",
    "            x = self.input_activation(layer(x))\n",
    "        x = self.layers[n_layer](x)\n",
    "        return x\n",
    "\n",
    "    def get_dist_to_init(self):\n",
    "        dists = []\n",
    "        for layer, init in zip(self.layers, self.initial_weights):\n",
    "            layer_dif = ch.norm(layer.weight.detach().cpu() - init.detach().cpu()).detach().item()\n",
    "            dists.append(layer_dif)\n",
    "        layer_2_dif = ch.norm(self.output_layer.weight.detach().cpu() - self.output_init.detach().cpu()).detach().item()\n",
    "        dists.append(layer_2_dif)\n",
    "        return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ch.no_grad\n",
    "def evaluate(model, dloader, loss_fn=None):\n",
    "  \n",
    "  model.eval()\n",
    "\n",
    "  acc = 0\n",
    "  loss = 0\n",
    "  nsamples = 0\n",
    "  nbatch = 0\n",
    "  \n",
    "  for inputs, targets in dloader:\n",
    "      \n",
    "      inputs = inputs.cuda()\n",
    "      targets = targets.cuda()\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      if loss_fn is not None:\n",
    "        loss += loss_fn(outputs, targets).item()\n",
    "        nbatch += 1\n",
    "              \n",
    "      acc += ch.sum(targets == outputs.argmax(dim=-1)).cpu()\n",
    "      nsamples += outputs.shape[0]\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  return acc/nsamples, loss/nbatch\n",
    "\n",
    "@ch.no_grad\n",
    "def evaluate_local_data_rank(model, dloader, layer_number, n_batch):\n",
    "  \n",
    "  model.eval()\n",
    "  \n",
    "  ranks_all = []\n",
    "  tvs_all = []\n",
    "  for inputs, _ in dloader:\n",
    "     #ims = ch.autograd.Variable(ims.cuda(), requires_grad=True)\n",
    "     #jacobians_batch = jacfwd(lambda x: model.forward_to(x, layer_number).sum(0))(ims.cuda())\n",
    "     func = jacfwd(lambda x: model.forward_to(x, layer_number))\n",
    "     vmaped_func = vmap(func, in_dims = 0, out_dims = 0)\n",
    "\n",
    "     jacobians_batch = vmaped_func(inputs.cuda())\n",
    "\n",
    "     #print(jacobians_batch.shape)   \n",
    "     #jacobians_batch = einops.rearrange(jacobians_batch, 'b 1 1 o 1 x y -> b o (x y)')\n",
    "\n",
    "     rank_batch = ch.linalg.matrix_rank(jacobians_batch, atol = .01)\n",
    "     tv_batch = ch.linalg.norm(jacobians_batch, dim = (1,2))\n",
    "\n",
    "     tvs_all.append(tv_batch)\n",
    "     ranks_all.append(rank_batch)\n",
    "\n",
    "     if len(ranks_all) > n_batch:\n",
    "       break\n",
    "     \n",
    "  ranks_all = ch.cat(ranks_all, dim=0).to(ch.float)\n",
    "  tvs_all = ch.cat(tvs_all, dim=0).to(ch.float)\n",
    "  rank_average = ranks_all.mean().item()\n",
    "  tvs_all = tvs_all.mean().item()\n",
    "  model.train()\n",
    "  \n",
    "  return rank_average, tvs_all\n",
    "\n",
    "@ch.no_grad\n",
    "def compute_local_complexity(model, dloader, config):\n",
    "  def add_noise_to_bias(model, sigma):\n",
    "    \"\"\"\n",
    "    Clones the model and adds Gaussian noise with variance sigma to all the bias terms.\n",
    "    \"\"\"\n",
    "\n",
    "    model_clone = MLP(input_dim = config.input_dim, hidden_dim = config.hidden_dim, output_dim = config.output_dim, n_layers=config.n_layers, input_std_init=config.input_weights_init_scale, output_std_init=config.output_weights_init_scale)\n",
    "\n",
    "    model_clone.load_state_dict(model.state_dict())\n",
    "\n",
    "    for layer in model_clone.layers:\n",
    "      if isinstance(layer, nn.Linear):\n",
    "        noise = ch.randn_like(layer.bias) * sigma\n",
    "        layer.bias.data += noise\n",
    "    return model_clone\n",
    "  \n",
    "  def normal_distribution_pdf(x, sigma):\n",
    "    return ch.exp(-((x) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "  def evaluate_layer_jac(model, l, x_in, sigma):\n",
    "    \"\"\"\n",
    "    Computes summand of local compelxity estimator for a given layer l\n",
    "    \"\"\"\n",
    "    #ims = ch.autograd.Variable(ims.cuda(), requires_grad=True)\n",
    "    #jacobians_batch = jacfwd(lambda x: model.forward_to(x, layer_number).sum(0))(ims.cuda())\n",
    "    #ims = rearrange(ims, 'b h w -> b (h w)')\n",
    "    func = jacfwd(lambda x: model.forward_to(x, l))\n",
    "    vmaped_func = vmap(func, in_dims = 0, out_dims = 0)\n",
    "\n",
    "    jacobians_batch = vmaped_func(x_in)\n",
    "    dist_from_bias = model.forward_to(x_in, l) - model.layers[l].bias\n",
    "    bias_term = normal_distribution_pdf(dist_from_bias, sigma = sigma)\n",
    "\n",
    "    neuron_norms_sum = []\n",
    "\n",
    "    for i in range(jacobians_batch.shape[1]): #iter over neurons\n",
    "      neuron_norms = jacobians_batch[:,i].norm(dim=1)\n",
    "      summand = bias_term[:,i] * neuron_norms\n",
    "      neuron_norms_sum.append(summand)\n",
    "\n",
    "    neuron_norms_sum  = ch.stack(neuron_norms_sum)\n",
    "\n",
    "    return neuron_norms_sum.sum(axis = 0)\n",
    "   \n",
    "  # Done with Helpers\n",
    "  running_mean = []\n",
    "  for inputs, _ in dloader:\n",
    "    to_stack_2 = []\n",
    "    x = inputs.cuda()\n",
    "\n",
    "    # Iterate over choices for biases\n",
    "    for _ in range(config.n_iters_LC):\n",
    "      to_stack = []\n",
    "\n",
    "      model_copy = add_noise_to_bias(model, config.sigma).to('cuda')\n",
    "\n",
    "      # Iterate over layers\n",
    "      for l in range(config.n_layers):\n",
    "        neuron_norm_sum_per_x = evaluate_layer_jac(model_copy, l, x, config.sigma)\n",
    "        to_stack.append(neuron_norm_sum_per_x)\n",
    "\n",
    "      del model_copy\n",
    "\n",
    "      to_stack = ch.stack(to_stack)\n",
    "      sum_x = to_stack.sum(axis = 0)\n",
    "      to_stack_2.append(sum_x)\n",
    "\n",
    "    to_stack_2 = ch.stack(to_stack_2)\n",
    "\n",
    "    to_stack_2 = to_stack_2.sum(axis = (0,1)) / (config.n_iters_LC * len(inputs))\n",
    "    running_mean.append(to_stack_2.item())\n",
    "\n",
    "  running_mean = ch.tensor(running_mean).mean().item()\n",
    "  return running_mean\n",
    "\n",
    "def evaluate_adv(model, dloader, config):\n",
    "\n",
    "  atk = PGD(model,\n",
    "          eps=config.atk_eps,\n",
    "          alpha=config.atk_alpha,\n",
    "          steps=config.atk_itrs,\n",
    "          dmin=config.dmin,\n",
    "          dmax=config.dmax\n",
    "          )\n",
    "\n",
    "  acc = 0\n",
    "  nsamples = 0\n",
    "  for inputs, targets in tqdm(dloader, desc=f\"Computing robust acc for eps:{config.atk_eps:.3f}\"):\n",
    "\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    adv_images = atk(inputs, targets)\n",
    "\n",
    "    with ch.no_grad():\n",
    "        adv_pred = model(adv_images).argmax(dim=-1)\n",
    "\n",
    "    acc += ch.sum(targets == adv_pred).cpu()\n",
    "    nsamples += adv_pred.shape[0]\n",
    "\n",
    "  return acc/nsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loaders, config):\n",
    "    print('Training....')\n",
    "    print(f'Logging at steps: {config.log_steps}')\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    # No Weight Decay on Biases\n",
    "    decay = dict()\n",
    "    no_decay = dict()\n",
    "    for name, param in model.named_parameters():\n",
    "        print('checking {}'.format(name))\n",
    "        if 'weight' in name:\n",
    "            decay[name] = param\n",
    "        else:\n",
    "            no_decay[name] = param\n",
    "        \n",
    "    print(f\"Weight Decay on: {decay.keys()}\")\n",
    "    print(f\"Weight Decay off: {no_decay.keys()}\")\n",
    "\n",
    "    # Optimizer Setup\n",
    "    \n",
    "    if config.optimizer == 'sgd':\n",
    "        print('Using SGD optimizer')\n",
    "        if not config.bias_decay:\n",
    "            opt = SGD([\n",
    "                {'params': no_decay.values(), 'weight_decay': 0.0},\n",
    "                {'params': decay.values(), 'weight_decay': config.weight_decay}\n",
    "            ],\n",
    "            lr=config.lr,\n",
    "            momentum=config.momentum)\n",
    "        else:\n",
    "            opt = SGD(model.parameters(),\n",
    "                  lr=config.lr,\n",
    "                  momentum=config.momentum,\n",
    "                  weight_decay=config.weight_decay)\n",
    "\n",
    "    elif config.optimizer == 'adam':\n",
    "        if not config.bias_decay:\n",
    "            opt = AdamW([\n",
    "                {'params': no_decay.values(), 'weight_decay': 0.0},\n",
    "                {'params': decay.values(), 'weight_decay': config.weight_decay}\n",
    "            ],\n",
    "            lr=config.lr)\n",
    "        else:   \n",
    "            opt = AdamW(model.parameters(),\n",
    "                    lr=config.lr,\n",
    "                    weight_decay=config.weight_decay)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    iters_per_epoch = len(loaders['train'])\n",
    "    epochs = np.floor(config.num_steps/iters_per_epoch)\n",
    "    print(f\"Training for {epochs} epochs\")\n",
    "\n",
    "    loss_fn = ch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_step = 0\n",
    "\n",
    "    while True:\n",
    "        if train_step > config.num_steps: break\n",
    "\n",
    "        for input, targets in loaders['train']:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            input = input.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            out = model(input)\n",
    "\n",
    "            loss = loss_fn(out, targets)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_step += 1\n",
    "\n",
    "            # Calculate Stats\n",
    "            print('Logging Evaluations')\n",
    "            if train_step in config.log_steps:\n",
    "                model.eval()\n",
    "\n",
    "                train_acc, train_loss = evaluate(model,\n",
    "                                                 loaders['train'],\n",
    "                                                 loss_fn)\n",
    "                test_acc, test_loss = evaluate(model,\n",
    "                                                 loaders['test'],\n",
    "                                                 loss_fn)\n",
    "                \n",
    "                l2_norm = 0\n",
    "                for name, param in model.named_parameters():\n",
    "                    l2_norm += ch.norm(param)\n",
    "\n",
    "                if config.compute_LC:\n",
    "                    print('Computing Local Complexity')\n",
    "                    train_LC = compute_local_complexity(model, loaders['train'], config)\n",
    "                    test_LC = compute_local_complexity(model, loaders['test'], config)\n",
    "                else:\n",
    "                    train_LC = 0\n",
    "                    test_LC = 0\n",
    "                    \n",
    "\n",
    "                if config.local_rank_eval:\n",
    "                    print('Computing Local Rank')\n",
    "                    local_ranks = []\n",
    "                    for layer_number in range(0, config.n_layers):\n",
    "                        local_rank, tv = evaluate_local_data_rank(model, loaders['test'], layer_number, config.n_batchs_for_local_rank)\n",
    "                        local_ranks.append(local_rank)\n",
    "\n",
    "                if config.compute_robust:\n",
    "                    print('Computing Robustness')\n",
    "                    robust_acc = evaluate_adv(model, loaders['test'], config)\n",
    "\n",
    "                stats_dict = {\n",
    "                    'iter': train_step,\n",
    "                    'train/loss': train_loss,\n",
    "                    'train/acc': train_acc,\n",
    "                    'test/loss': test_loss,\n",
    "                    'test/acc': test_acc,\n",
    "                    'adv/acc': robust_acc,\n",
    "                    'l2_norm': l2_norm,\n",
    "                    'train/LC': train_LC,\n",
    "                    'test/LC': test_LC,\n",
    "                    'TV': tv,\n",
    "                } | {f'train/local_rank_{i}': rank for i, rank in enumerate(local_ranks)}\n",
    "\n",
    "                wandb.log(stats_dict)\n",
    "\n",
    "                if config.save_model:\n",
    "                    ch.save(model.state_dict(), os.path.join(config.model_save_dir, f'model_{train_step}.pth'))\n",
    "\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ba711",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c411e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mniketpatel\u001b[0m (\u001b[33mniket\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/local_complexity/wandb/run-20250520_074400-jzderz0v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/niket/mnist-analysis/runs/jzderz0v' target=\"_blank\">dainty-voice-1</a></strong> to <a href='https://wandb.ai/niket/mnist-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/niket/mnist-analysis' target=\"_blank\">https://wandb.ai/niket/mnist-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/niket/mnist-analysis/runs/jzderz0v' target=\"_blank\">https://wandb.ai/niket/mnist-analysis/runs/jzderz0v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MLP with 4 layers and 200 hidden dimensions\n",
      "Training....\n",
      "Logging at steps: [      1       2       3       4       5       7       9      12      16\n",
      "      22      29      39      51      68      91     120     159     212\n",
      "     281     372     494     655     868    1151    1526    2023    2682\n",
      "    3556    4714    6250    8286   10985   14563   19306   25595   33932\n",
      "   44984   59636   79060  104811  138949  184206  244205  323745  429193\n",
      "  568986  754312 1000000]\n",
      "checking layers.0.weight\n",
      "checking layers.0.bias\n",
      "checking layers.1.weight\n",
      "checking layers.1.bias\n",
      "checking layers.2.weight\n",
      "checking layers.2.bias\n",
      "checking layers.3.weight\n",
      "checking layers.3.bias\n",
      "checking output_layer.weight\n",
      "checking output_layer.bias\n",
      "Weight Decay on: dict_keys(['layers.0.weight', 'layers.1.weight', 'layers.2.weight', 'layers.3.weight', 'output_layer.weight'])\n",
      "Weight Decay off: dict_keys(['layers.0.bias', 'layers.1.bias', 'layers.2.bias', 'layers.3.bias', 'output_layer.bias'])\n",
      "Training for 62500.0 epochs\n",
      "Logging Evaluations\n",
      "Computing Local Complexity\n",
      "Computing Local Rank\n",
      "Computing Robustness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing robust acc for eps:0.314: 100%|██████████| 16/16 [00:00<00:00, 73.77it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Evaluations\n",
      "Computing Local Complexity\n",
      "Computing Local Rank\n",
      "Computing Robustness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing robust acc for eps:0.314: 100%|██████████| 16/16 [00:00<00:00, 68.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Evaluations\n",
      "Computing Local Complexity\n",
      "Computing Local Rank\n",
      "Computing Robustness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing robust acc for eps:0.314: 100%|██████████| 16/16 [00:00<00:00, 75.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Evaluations\n",
      "Computing Local Complexity\n",
      "Computing Local Rank\n",
      "Computing Robustness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing robust acc for eps:0.314: 100%|██████████| 16/16 [00:00<00:00, 69.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Evaluations\n",
      "Computing Local Complexity\n",
      "Computing Local Rank\n",
      "Computing Robustness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing robust acc for eps:0.314: 100%|██████████| 16/16 [00:00<00:00, 69.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Evaluations\n",
      "Logging Evaluations\n",
      "Computing Local Complexity\n",
      "Computing Local Rank\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m loaders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loader,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: test_loader\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m train(model, loaders, config)\n",
      "Cell \u001b[0;32mIn[8], line 107\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loaders, config)\u001b[0m\n\u001b[1;32m    105\u001b[0m     local_ranks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_number \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, config\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 107\u001b[0m         local_rank, tv \u001b[38;5;241m=\u001b[39m evaluate_local_data_rank(model, loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], layer_number, config\u001b[38;5;241m.\u001b[39mn_batchs_for_local_rank)\n\u001b[1;32m    108\u001b[0m         local_ranks\u001b[38;5;241m.\u001b[39mappend(local_rank)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcompute_robust:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mevaluate_local_data_rank\u001b[0;34m(model, dloader, layer_number, n_batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m jacobians_batch \u001b[38;5;241m=\u001b[39m vmaped_func(inputs\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#print(jacobians_batch.shape)   \u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#jacobians_batch = einops.rearrange(jacobians_batch, 'b 1 1 o 1 x y -> b o (x y)')\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m rank_batch \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatrix_rank(jacobians_batch, atol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.01\u001b[39m)\n\u001b[1;32m     47\u001b[0m tv_batch \u001b[38;5;241m=\u001b[39m ch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(jacobians_batch, dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     49\u001b[0m tvs_all\u001b[38;5;241m.\u001b[39mappend(tv_batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# init wandb\n",
    "wandb.init(project=\"mnist-analysis\", config=config)\n",
    "\n",
    "# Create model based on config\n",
    "def create_model(config):\n",
    "    model = MLP(\n",
    "        input_dim=784,  # MNIST images are 28x28 = 784 pixels\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        output_dim=10,  # 10 classes for MNIST\n",
    "        n_layers=config.n_layers\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = create_model(config)\n",
    "print(f\"Created MLP with {config.n_layers} layers and {config.hidden_dim} hidden dimensions\")\n",
    "\n",
    "# Create data loaders dictionary\n",
    "loaders = {\n",
    "    'train': train_loader,\n",
    "    'test': test_loader\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "train(model, loaders, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0031a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
